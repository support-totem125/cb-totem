# ü§ñ Configuraci√≥n de Ollama con n8n

## üìã ¬øQu√© es Ollama?

**Ollama** es un servidor local que ejecuta modelos de IA de c√≥digo abierto sin necesidad de Internet ni credenciales. Perfecto para:
- ‚úÖ Agentes de IA sin costos
- ‚úÖ Procesamiento local y privado
- ‚úÖ Integraci√≥n con n8n
- ‚úÖ Alternativa a OpenAI

## üöÄ Instalaci√≥n y Configuraci√≥n

### 1. Levantar Ollama

```bash
cd /home/admin/Documents/chat-bot-totem
docker compose up -d ollama
```

### 2. Esperar a que Ollama est√© listo

```bash
sleep 10
docker logs ollama -f
```

Ver√°s algo como:
```
time=2025-10-27T10:00:00.000Z level=info msg="Ollama is running"
```

### 3. Descargar un Modelo (La primera vez tarda)

Hay varias opciones seg√∫n tu RAM disponible:

#### üü¢ **Ligero (4GB RAM)** - Recomendado
```bash
docker exec ollama ollama pull mistral
```

#### üü° **Medio (6GB RAM)**
```bash
docker exec ollama ollama pull neural-chat
```

#### üî¥ **Pesado (8GB+ RAM)**
```bash
docker exec ollama ollama pull dolphin-mixtral
```

#### üîµ **Cl√°sico (4GB RAM)**
```bash
docker exec ollama ollama pull llama2
```

### 4. Verificar que el modelo est√° descargado

```bash
docker exec ollama ollama list
```

Deber√≠as ver algo como:
```
NAME           ID              SIZE   MODIFIED
mistral:latest xxxxxxxx...     4.1 GB 1 minute ago
```

### 5. Probar Ollama localmente

```bash
# Desde tu m√°quina
curl http://192.168.1.74:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Hola, ¬øc√≥mo est√°s?",
  "stream": false
}'
```

Deber√≠as recibir una respuesta JSON con la IA respondiendo.

---

## üîó Integraci√≥n con n8n

### Opci√≥n A: Usar HTTP Request Node (M√°s control)

1. En n8n, crea un nodo **"HTTP Request"**
2. Configura:
   - **Method**: POST
   - **URL**: `http://ollama:11434/api/generate`
   - **Body** (JSON):
     ```json
     {
       "model": "mistral",
       "prompt": "Tu prompt aqu√≠",
       "stream": false
     }
     ```

3. Procesa la respuesta con un nodo **"Function"** para extraer el texto

### Opci√≥n B: Usar OpenAI-Compatible API (Mejor)

Ollama tiene un endpoint compatible con OpenAI. En n8n:

1. Crea un nodo **"AI Agent"** o **"OpenAI"**
2. Configura como URL: `http://ollama:11434/v1`
3. Modelo: `mistral` (o el que descargaste)
4. API Key: `dummy` (Ollama no requiere clave)

---

## üìä Modelos Disponibles

| Modelo          | Tama√±o | Velocidad | Memoria | Mejor para                     |
| --------------- | ------ | --------- | ------- | ------------------------------ |
| mistral         | 4.1 GB | R√°pido    | 4GB     | Prop√≥sito general, recomendado |
| neural-chat     | 4.1 GB | R√°pido    | 4GB     | Conversaci√≥n natural           |
| llama2          | 3.8 GB | Medio     | 4GB     | Textos extensos                |
| dolphin-mixtral | 26 GB  | Lento     | 8GB+    | Tarea complejas                |
| openchat        | 3.9 GB | R√°pido    | 4GB     | Chat r√°pido                    |

**Recomendaci√≥n**: Comienza con `mistral` - es r√°pido y de buena calidad.

---

## üîß Configuraci√≥n en n8n

### Crear un Workflow con Ollama

1. **Nodo 1: Webhook** (entrada de datos)
2. **Nodo 2: HTTP Request** a Ollama
   ```
   POST http://ollama:11434/api/generate
   Body: {
     "model": "mistral",
     "prompt": "{{ $json.message }}",
     "stream": false
   }
   ```
3. **Nodo 3: Function** (procesar respuesta)
   ```javascript
   return {
     json: {
       response: JSON.parse(item.json.response).response
     }
   };
   ```

### Variables √ötiles
```
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=mistral
```

---

## üêõ Troubleshooting

### ‚ùå "Connection refused"
```bash
# Verificar que Ollama est√° corriendo
docker ps | grep ollama

# Ver logs
docker logs ollama
```

### ‚ùå "Model not found"
```bash
# Ver modelos disponibles
docker exec ollama ollama list

# Descargar modelo faltante
docker exec ollama ollama pull mistral
```

### ‚ùå "Out of memory"
```bash
# Reducir recursos o usar modelo m√°s ligero
docker exec ollama ollama pull neural-chat
```

---

## üìà Performance Tips

1. **En docker-compose.yaml** ya est√° limitado a 6GB, aumentar si es necesario
2. **Usar GPU** (si disponible, agregar `--gpus all` en docker-compose)
3. **Modelos peque√±os** para menor latencia
4. **Cach√© de respuestas** en n8n para reducir llamadas

---

## üîå URLs √ötiles

| Servicio   | URL                         |
| ---------- | --------------------------- |
| Ollama API | `http://192.168.1.74:11434` |
| n8n        | `http://192.168.1.74:5678`  |
| Chatwoot   | `http://192.168.1.74:3000`  |

---

## üìù Ejemplo Completo: Chatbot en Chatwoot con Ollama

1. Mensaje llega a Chatwoot
2. Webhook dispara n8n
3. n8n env√≠a mensaje a Ollama
4. Ollama genera respuesta
5. n8n env√≠a respuesta a Chatwoot
6. Usuario ve respuesta autom√°tica

**¬°Con esto tienes un chatbot de IA sin costos!** üöÄ

